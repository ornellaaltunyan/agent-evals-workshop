"""Create an online scorer in Braintrust that evaluates SQL on the run_sql_query span."""

from dotenv import load_dotenv

load_dotenv()

import braintrust

PROJECT = "agent-evals-workshop"
SCORER_NAME = "SQL Quality"
SCORER_SLUG = "sql-quality-scorer"

SQL_SCORER_SYSTEM_PROMPT = """\
You are evaluating the quality of a SQL query generated by an AI agent for an NBA analytics database.

The agent was given this question: {{input.input_message}}
The agent generated this SQL query: {{input.query}}
It generated this result: {{input.output}}

Assess the SQL query based on:
- Relevance to the user's question
- Syntactic validity (well-formed SQL)
- Semantic correctness (likely to answer the question accurately)
- Appropriate use of SQL constructs (JOINs, aggregations, date filters, etc.)
- Quality of the returned data

Based on the provided question and the result it produced, if the query appears to be appropriately written to answer the user's question, return a score of 1. 

If not, return a score of 0. 

Return JSON with exactly this format:
{
    "score": 0 or 1,
    "name": "sql_quality",
    "metadata": {
        "rationale": "<brief explanation>"
    }
}
"""


def get_project_id(conn):
    resp = conn.get_json("/v1/project", {"project_name": PROJECT})
    for obj in resp.get("objects", []):
        if obj["name"] == PROJECT:
            return obj["id"]
    return None


def create_scorer_function(conn, project_id):
    print(f"Uploading scorer function '{SCORER_NAME}'...")
    result = conn.post_json(
        "/v1/function",
        {
            "project_id": project_id,
            "name": SCORER_NAME,
            "slug": SCORER_SLUG,
            "function_type": "scorer",
            "function_data": {"type": "prompt"},
            "prompt_data": {
                "prompt": {
                    "type": "chat",
                    "messages": [
                        {"role": "system", "content": SQL_SCORER_SYSTEM_PROMPT},
                    ],
                },
                "options": {
                    "model": "gpt-5-mini",
                    "params": {
                        "response_format": {"type": "json_object"},
                    },
                },
            },
        },
    )
    print(f"Scorer function '{SCORER_NAME}' uploaded (id={result['id']}).")
    return result["id"]


def create_online_scorer(conn, project_id, scorer_function_id):
    existing = conn.get_json("/v1/project_score", {"project_name": PROJECT})
    for score in existing.get("objects", []):
        if score["name"] == SCORER_NAME:
            print(f"Online scorer '{SCORER_NAME}' already exists, skipping.")
            return

    print(f"Creating online scorer '{SCORER_NAME}' on span 'run_sql_query'...")
    conn.post_json(
        "/v1/project_score",
        {
            "project_id": project_id,
            "name": SCORER_NAME,
            "score_type": "online",
            "config": {
                "online": {
                    "sampling_rate": 1.0,
                    "scorers": [{"type": "function", "id": scorer_function_id}],
                    "apply_to_root_span": False,
                    "apply_to_span_names": ["run_sql_query"],
                }
            },
        },
    )
    print(f"Online scorer '{SCORER_NAME}' created.")


def run():
    braintrust.login()
    conn = braintrust.api_conn()
    project_id = get_project_id(conn)
    scorer_function_id = create_scorer_function(conn, project_id)
    create_online_scorer(conn, project_id, scorer_function_id)


if __name__ == "__main__":
    run()
